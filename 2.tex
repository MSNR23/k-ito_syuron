\chapter[強化学習を用いた投擲フォーム導出]{強化学習を用いた投擲フォーム導出}

\section{はじめに}
投擲フォームの導出方法について、本研究では強化学習による最適化手法を採用する。
投擲フォームは個人・競技によって異なる。また、多くの要素が同時に影響する複雑な動作であり、
時々刻々と全身の運動連鎖によるダイナミクスが変化するため、明示的な解を求めることは困難である。
強化学習は複雑なモデルや学習プロセスにより最適化までに時間的計算コスト等はかかるが、
明示的な解がなく詳細なパラメータ設定が求められる投擲フォーム導出において有効な手段である。
\section{強化学習}
\subsection{強化学習}
強化学習とは、環境を観測しながらエージェントが試行錯誤することで、獲得する方策を最適化する機械学習の手法である。
方策とは、環境に対してエージェントがどのような行動を選択すべきかを決定する部分である。
強化学習の流れの概略図について、図～に示す。
エージェントが環境に対して行動を選択し、実行する。環境はその行動に対しエージェントに状態と報酬をフィードバックする。
その後、エージェントは受け取った状態から報酬がより大きくなるような行動を選択する。
この学習を繰り返すことでエージェントは行動を改善し、最適な行動を見つけることができる。
\subsection{強化学習手法}
本研究では、強化学習の手法の1つであるQ学習を採用する。Q学習とは、ある状態におけるある行動の価値をQ値と呼ばれる価値観する鵜を用いて表す。
このQ値は短期的な報酬でなく、より報酬の高い状態へと遷移できるかを見据え、行動の長期的な価値を示すものである。\\
Q学習において、Q値は\equref{Q}で更新する。\\
\begin{eqnarray}
  \equlabel{Q}
  Q(s,a)=(1-\alpha)Q(s,a)+\alpha(r+\gamma maxQ(s',a'))
\end{eqnarray}
\equref{Q}において、$\alpha$は学習率、$\gamma$は割引率であり、範囲はそれぞれ0$\leq$$\alpha$$\leq$1、0$\leq$$\gamma$$\leq$1である。また、$Q(s,a)$は状態が$s$における行動$a$の行動価値関数である。
学習率はQ値の更新の大きさを決定するパラメータであり、\equref{Q}におけるQ値の更新量であるTD誤差$\alpha(r+\gamma maxQ(s',a'))$に影響する。学習率が極端に大きすぎるとQ値の更新が過剰となるため学習が発散する。
一方、学習率が極端に小さすぎると学習の収束が遅くなり、局所的な最適解にとどまる可能性がある。そのため、Q学習において最適な学習率を設定することが重要である。
割引率は将来の価値をどれだけ割り引くのかを決定するパラメータである。割引率が大きいほど長期的な報酬を重要視するため学習の収束に時間を要する。一方、割引率が小さいほど短期的な報酬を重要視するため、最適解に収束しない可能性がある。
そのため、学習率と同様に最適な割引率を設定することが重要である。

Q値を用いる学習手法として、Q学習のほかにSARSAなどが挙げられる。

\subsection{行動空間}
\subsection{行動選択法}

