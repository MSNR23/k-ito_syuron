\chapter[強化学習を用いた投擲フォーム導出]{強化学習を用いた投擲フォーム導出}

\section{はじめに}
投擲フォームの導出方法について，本研究では強化学習による最適化手法を採用する．\\
投擲フォームは競技・個人によって異なる．また，多くの要素が同時に影響する複雑な動作であり，時々刻々と全身の運動連鎖によるダイナミクスが変化するため，明示的な解を求めることは困難である．強化学習は複雑なモデルや学習プロセスにより最適化までに時間的計算コスト等はかかるが，明示的な解がなく詳細なパラメータ設定が求められる投擲フォーム導出において有効な手段である．

\section{強化学習}
\subsection{強化学習}
強化学習とは，環境を観測しながらエージェントが試行錯誤することで，獲得する方策を最適化する機械学習の手法である．
方策とは，環境に対してエージェントがどのような行動を選択すべきかを決定する部分である．
強化学習の流れの概略図について，\figref{2.1.jpg}に示す．
\fig{2.1.jpg}{width=1.0\hsize}{How Reinforcement Learning Works}
エージェントが環境に対して行動を選択し，実行する．環境はその行動に対しエージェントに状態と報酬をフィードバックする．
その後，エージェントは受け取った状態から報酬がより大きくなるような行動を選択する．
この学習を繰り返すことでエージェントは行動を改善し，最適な行動を見つけることができる．
\subsection{強化学習手法}
本研究では，強化学習の手法の1つであるQ学習を採用する．Q学習とは，ある状態における，ある行動の価値をQ値と呼ばれる価値関数を用いて表す．
このQ値は短期的な報酬でなく，より報酬の高い状態へと遷移できるかを見据え，行動の長期的な価値を示すものである．\\
Q学習において，Q値は\equref{Q}で更新する．\\
\begin{eqnarray}
  \equlabel{Q}
  Q(s,a)=(1-\alpha)Q(s,a)+\alpha(r+\gamma maxQ(s',a'))
\end{eqnarray}
\equref{Q}において，$\alpha$は学習率，$\gamma$は割引率であり，範囲はそれぞれ0$\leq$$\alpha$$\leq$1，0$\leq$$\gamma$$\leq$1である．また，$Q(s,a)$は状態が$s$における行動$a$の行動価値関数である．\\
学習率はQ値の更新の大きさを決定するパラメータであり，\equref{Q}におけるQ値の更新量であるTD誤差$\alpha(r+\gamma maxQ(s',a'))$に影響する．学習率が極端に大きすぎるとQ値の更新が過剰となるため学習が発散する．一方，学習率が極端に小さすぎると学習の収束が遅くなり，局所的な最適解にとどまる可能性がある．そのため，Q学習において最適な学習率を設定することが重要である．\\
割引率は将来の価値をどれだけ割り引くのかを決定するパラメータである．割引率が大きいほど長期的な報酬を重要視するため学習の収束に時間を要する．一方，割引率が小さいほど短期的な報酬を重要視するため，最適解に収束しない可能性がある．\\
そのため，学習率と同様に最適な割引率を設定することが重要である．
\subsection{行動選択方法}
本研究では，行動選択方法として$\varepsilon$-greedy法を採用した．$\varepsilon$-greedy法は，探索と活用のトレードオフを管理するための一般的な強化学習の手法である．
$\varepsilon$-greedy法では，$\varepsilon$（0$\leq$$\varepsilon$$\leq$1）の確率で全ての行動からランダムに行動を選択し，1-$\varepsilon$の確率でルールの価値が最も高い行動を選択する．
学習が進んでも$\varepsilon$が大きいと，無駄な探索が増加し学習効率が低下する．そのため，学習が進むにつれて$\varepsilon$を0に近づけることで，より学習したルールの価値に基づいた行動が選択できる．
\subsection{その他}
本論文に登場する強化学習の用語について述べる．\\
エピソードとは，学習回数のことである．強化学習ではエピソードを重ねるごとにエージェントが行動を改善し，報酬が高くなるように行動していく．ステップとは，1エピソード内でエージェントと環境の相互作用が行われる回数のことである．時間ステップと1エピソード内のステップ数により，1エピソードでエージェントが行う時間を設定することができる．
